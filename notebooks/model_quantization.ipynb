{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faddc14d-3dd5-43be-babd-0d19d8c98e95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85bc6cbcb6f4f9e8b3400d4bf8658a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True\n",
    ")\n",
    "\n",
    "model_folder = \"/mnt/resource/blobfuse2tmp/nando-mlops-vision-prod/models/office/nando-eye/experiments/minicpm-v-2_6/20240903_2/output/merged_model/\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_folder, quantization_config = quant_config, trust_remote_code=True, low_cpu_mem_usage=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_folder, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87669963-4708-4723-a565-2cb2ec6405f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MiniCPMV(\n",
       "  (llm): Qwen2ForCausalLM(\n",
       "    (model): Qwen2Model(\n",
       "      (embed_tokens): Embedding(151666, 3584)\n",
       "      (layers): ModuleList(\n",
       "        (0-27): 28 x Qwen2DecoderLayer(\n",
       "          (self_attn): Qwen2SdpaAttention(\n",
       "            (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "            (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "            (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
       "            (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
       "            (rotary_emb): Qwen2RotaryEmbedding()\n",
       "          )\n",
       "          (mlp): Qwen2MLP(\n",
       "            (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "            (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
       "            (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
       "            (act_fn): SiLU()\n",
       "          )\n",
       "          (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "          (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "        )\n",
       "      )\n",
       "      (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
       "    )\n",
       "    (lm_head): Linear(in_features=3584, out_features=151666, bias=False)\n",
       "  )\n",
       "  (vpm): SiglipVisionTransformer(\n",
       "    (embeddings): SiglipVisionEmbeddings(\n",
       "      (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
       "      (position_embedding): Embedding(4900, 1152)\n",
       "    )\n",
       "    (encoder): SiglipEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-26): 27 x SiglipEncoderLayer(\n",
       "          (self_attn): SiglipAttention(\n",
       "            (k_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "            (v_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "            (q_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "            (out_proj): Linear4bit(in_features=1152, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): SiglipMLP(\n",
       "            (activation_fn): PytorchGELUTanh()\n",
       "            (fc1): Linear4bit(in_features=1152, out_features=4304, bias=True)\n",
       "            (fc2): Linear4bit(in_features=4304, out_features=1152, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (resampler): Resampler(\n",
       "    (kv_proj): Linear4bit(in_features=1152, out_features=3584, bias=False)\n",
       "    (attn): MultiheadAttention(\n",
       "      (out_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
       "    )\n",
       "    (ln_q): LayerNorm((3584,), eps=1e-06, elementwise_affine=True)\n",
       "    (ln_kv): LayerNorm((3584,), eps=1e-06, elementwise_affine=True)\n",
       "    (ln_post): LayerNorm((3584,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b80f05c5-2cc4-41df-9081-b1edb3441edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.0820789337158203 seconds\n",
      "Nothing\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "question_train = 'Which object is shown to the camera? Answer the question using a single word or phrase.'\n",
    "\n",
    "image = Image.open('9ef0376d1227b1f2_03_09_2024__16_59_25.jpg').convert('RGB')\n",
    "question = 'You are an expert in recognizing objects. The objects are showned to the camera by persons. Which object is showned to the camera?. Please answer with one word and not consider hands.'\n",
    "msgs = [{'role': 'user', 'content': question_train}]\n",
    "\n",
    "answer = model.chat(\n",
    "    image=image,\n",
    "    msgs=msgs,\n",
    "    context=None,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True\n",
    ")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3029e2e7-7f9b-49d0-9689-e12d017b9bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 7.594323635101318 seconds\n",
      "The first image shows a close-up of a person's hand holding a small, dark-colored object against their face. The object appears to be a smartphone or a similar device with a screen that is turned off or not in use.\n",
      "\n",
      "The second image depicts a large, open field under a clear blue sky during what seems to be daytime. There are no visible buildings, trees, or other distinct features in the landscape, giving it an expansive and somewhat desolate appearance.\n"
     ]
    }
   ],
   "source": [
    "#Two images comparison\n",
    "import time\n",
    "\n",
    "# Start time\n",
    "start_time = time.time()\n",
    "\n",
    "question_train = 'Which object is shown to the camera? Answer the question using a single word or phrase.'\n",
    "question_compare = 'What can you see in the two images?'\n",
    "question = 'You are an expert in recognizing objects. The objects are showned to the camera by persons. Which object is showned to the camera?. Please answer with one word and not consider hands.'\n",
    "\n",
    "image = Image.open('2697d1c510a42b54_03_09_2024__16_27_50.jpg').convert('RGB')\n",
    "image_2 = Image.open('9ef0376d1227b1f2_03_09_2024__16_59_25.jpg').convert('RGB')\n",
    "msgs = [{'role': 'user', 'content': question_compare}]\n",
    "\n",
    "answer = model.chat(\n",
    "    image=[image, image_2],\n",
    "    msgs=msgs,\n",
    "    context=None,\n",
    "    tokenizer=tokenizer,\n",
    "    sampling=True\n",
    ")\n",
    "\n",
    "# End time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and print the execution time\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6307822e-ce90-4987-91a0-7ac58a37e676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plastic object\n"
     ]
    }
   ],
   "source": [
    "#Few show tuning\n",
    "\n",
    "question = \"object in picture\" \n",
    "image1 = Image.open('2697d1c510a42b54_03_09_2024__16_27_50.jpg').convert('RGB')\n",
    "answer1 = \"watch\"\n",
    "image2 = Image.open('9ef0376d1227b1f2_03_09_2024__16_59_25.jpg').convert('RGB')\n",
    "answer2 = \"wire\"\n",
    "image_test = Image.open('c037acfee1068676_03_09_2024__17_25_51.jpg').convert('RGB')\n",
    "\n",
    "msgs = [\n",
    "    {'role': 'user', 'content': [image1, question]}, {'role': 'assistant', 'content': [answer1]},\n",
    "    {'role': 'user', 'content': [image2, question]}, {'role': 'assistant', 'content': [answer2]},\n",
    "    {'role': 'user', 'content': [image_test, question]}\n",
    "]\n",
    "\n",
    "answer = model.chat(\n",
    "    image=None,\n",
    "    msgs=msgs,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
